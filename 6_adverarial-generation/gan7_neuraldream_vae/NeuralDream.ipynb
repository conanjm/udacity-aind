{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating NeuralDream Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook walks through the process of creating Neural Dream Videos. \n",
    "The process is as follows:\n",
    "1. Download a video in mp4 format, and resize it to 128x128.\n",
    "2. Split video into individual frames.\n",
    "3. Train VAE using individual frames.\n",
    "4. Generate latent representation of all frames from trained VAE.\n",
    "5. Train RNN using latent representations.\n",
    "6. Generate new sequence of latent representations using RNN.\n",
    "7. Decode new latent sequence using trained VAE to get new series of frames.\n",
    "8. Combine new frame sequence into Neural Dream Video.\n",
    "\n",
    "The hyperparameters of the model are free to be adjusted, as the values here are only heuristics that worked well for me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import cPickle\n",
    "\n",
    "from model_rnn import Model\n",
    "\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import os\n",
    "from model_vae import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up project name and folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'video'\n",
    "\n",
    "input_path = './inputs/'+project_name\n",
    "output_path = './outputs/'+project_name\n",
    "vae_checkpoints = './vae_cp/'+project_name\n",
    "lstm_checkpoints = './lstm_cp/'+project_name\n",
    "paths = [input_path,output_path,vae_checkpoints,lstm_checkpoints]\n",
    "for path in paths:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing source video\n",
    "\n",
    "Resize the desired video to 128x128 using ffmpeg from the terminal or your preferred video processing software.\n",
    "\n",
    "`ffmpeg -i input.mp4 -vf scale=128:128 output.mp4`\n",
    "\n",
    "Next we use the smaller video to generate ten frames per second, and save them all to a frame folder.\n",
    "\n",
    "`ffmpeg -i output.mp4 -r 10 -f image2 ./inputs/project_name/%05d.png`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network_architecture = \\\n",
    "    dict(n_hidden_recog_1=256, # 1st layer encoder neurons\n",
    "         n_hidden_recog_2=256, # 2nd layer encoder neurons\n",
    "         n_hidden_gener_1=256, # 1st layer decoder neurons\n",
    "         n_hidden_gener_2=256, # 2nd layer decoder neurons\n",
    "         n_input=49152, # Number of values per video frame\n",
    "         n_z=64)  # dimensionality of latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the frames into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadImages(data):\n",
    "    images = []\n",
    "    for myFile in data:\n",
    "        img = Image.open(myFile)\n",
    "        images.append(np.reshape(np.array(img),[49152]))\n",
    "    images = np.array(images)\n",
    "    images = images.astype('float32')\n",
    "    images = images / 256\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataS = sorted(glob(os.path.join(\"./inputs/\", project_name, \"*.png\")))\n",
    "imagesS = loadImages(dataS)\n",
    "n_samples = len(imagesS)\n",
    "print 'Frames loaded. There are ',str(n_samples),'frames in project',project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_vae(network_architecture, learning_rate=1e-4,\n",
    "          batch_size=50, training_epochs=10, display_step=5,model_path='./vae_checkpoints'):\n",
    "    vae = VariationalAutoencoder(network_architecture, \n",
    "                                 learning_rate=learning_rate, \n",
    "                                 batch_size=batch_size,load_model = False,checkpoint_folder=model_path)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(n_samples / batch_size)\n",
    "        # Loop over all batches\n",
    "        perms = np.random.permutation(imagesS)\n",
    "        for i in range(perms.shape[0]/batch_size):\n",
    "            batch_xs = perms[i *batch_size:(i+1) * batch_size,:]\n",
    "\n",
    "            # Fit training using batch data\n",
    "            cost = vae.partial_fit(batch_xs)\n",
    "            # Compute average loss\n",
    "            avg_cost += cost / n_samples * batch_size\n",
    "\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \\\n",
    "                  \"cost=\", \"{:.9f}\".format(avg_cost)\n",
    "    vae.save_model(epoch,model_path)\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_size = 50\n",
    "vae = train_vae(network_architecture, training_epochs=100,model_path='./vae_cp/'+project_name,batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the reconstruction capacity of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sample = imagesS[0:batch_size]\n",
    "x_reconstruct = vae.reconstruct(x_sample)\n",
    "plt.figure(figsize=(8, 12))\n",
    "for i in range(5):\n",
    "\n",
    "    plt.subplot(5, 2, 2*i + 1)\n",
    "    plt.imshow((x_sample[i+0].reshape(128, 128,3)), vmin=0, vmax=1)\n",
    "    plt.title(\"Test input\")\n",
    "    plt.subplot(5, 2, 2*i + 2)\n",
    "    plt.imshow(x_reconstruct[i+0].reshape(128, 128,3), vmin=0, vmax=1)\n",
    "    plt.title(\"Reconstruction\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate latent representation of each frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_z = vae.transform(imagesS)\n",
    "with open('./data/'+project_name+'_Z.p','w') as f:\n",
    "    cPickle.dump(x_z,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model and loading latent representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args():\n",
    "    def __init__(self):\n",
    "        self.rnn_size = 256\n",
    "        self.num_layers = 2\n",
    "        self.model = 'lstm'\n",
    "        self.batch_size = 5\n",
    "        self.seq_length = 50\n",
    "        self.num_epochs = 150\n",
    "        self.save_every = 1000\n",
    "        self.grad_clip = 5.\n",
    "        self.learning_rate = 2e-2\n",
    "        self.decay_rate = 0.97\n",
    "        self.input_size = 64\n",
    "        self.save_dir = './lstm_cp/'+project_name\n",
    "        \n",
    "argsA = args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/'+project_name+'_Z.p','r') as f:\n",
    "    z_images = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_batches():\n",
    "    num_batches = len(z_images) / (argsA.batch_size * argsA.seq_length)\n",
    "    tensor = z_images[:num_batches * argsA.batch_size * argsA.seq_length]\n",
    "    xdata = tensor\n",
    "    ydata = np.copy(tensor)\n",
    "    ydata[:-1] = xdata[1:]\n",
    "    ydata[-1] = xdata[0]\n",
    "    x_batches = np.split(xdata.reshape(argsA.batch_size,argsA.seq_length, -1), num_batches, 2)\n",
    "    y_batches = np.split(ydata.reshape(argsA.batch_size,argsA.seq_length, -1), num_batches, 2)\n",
    "    return x_batches,y_batches,num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "argsA = args()\n",
    "model = Model(argsA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_batches,y_batches,num_batches = create_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "    batchPointer = 0\n",
    "    for e in xrange(argsA.num_epochs):\n",
    "        sess.run(tf.assign(model.lr, argsA.learning_rate * (argsA.decay_rate ** e)))\n",
    "        batchPointer = 0\n",
    "        state = model.initial_state.eval()\n",
    "        for b in xrange(num_batches):\n",
    "            start = time.time()\n",
    "            x, y = x_batches[b],y_batches[b]\n",
    "            feed = {model.input_data: x, model.targets: y, model.initial_state: state}\n",
    "            train_loss, state, _,myOut = sess.run([model.cost, model.final_state, model.train_op,model.logits], feed)\n",
    "            end = time.time()\n",
    "        print 'Loss at epoch',e,':',train_loss\n",
    "        if (e == argsA.num_epochs -1):\n",
    "            checkpoint_path = os.path.join(argsA.save_dir, 'model.ckpt')\n",
    "            saver.save(sess, checkpoint_path, global_step = e * num_batches)\n",
    "            print \"model saved to {}\".format(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating new sequences from RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model_generate = Model(argsA,True)\n",
    "\n",
    "x_batches,y_batches,num_batches = create_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These hyperparameters adjust the generation process\n",
    "frames_to_generate = 1000 # Numer of frames to generate\n",
    "noise_to_add = 0.1 # Amount of noise to add to each generated latent representation\n",
    "reset = False # By resetting the generation process periodically, we can prevent the RNN from getting stuck "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "    ckpt = tf.train.get_checkpoint_state(argsA.save_dir)\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    state = model_generate.initial_state.eval()\n",
    "    x = x_batches[0]\n",
    "    xs = []\n",
    "    for i in range(frames_to_generate):\n",
    "        feed = {model_generate.input_data: x, model_generate.initial_state: state}\n",
    "        state,x1 = sess.run([model_generate.final_state,model_generate.logits], feed)\n",
    "        xs.append(x1[0])\n",
    "        x = x1.reshape([1,1,64]) + np.random.uniform(-noise_to_add,noise_to_add,[1,1,64])\n",
    "        if i % 100 == 0 and reset == True:\n",
    "            state = model_generate.cell.zero_state(1, tf.float32).eval()\n",
    "            x = x_batches[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newXs = np.array(xs)\n",
    "with open('./data/new'+project_name+'_Z.p','w') as f:\n",
    "    cPickle.dump(newXs,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Generating video from new latent sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "vae = VariationalAutoencoder(network_architecture, \n",
    "                                 learning_rate=1e-3, \n",
    "                                 batch_size=200,load_model = True,checkpoint_folder='./vae_cp/'+project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/new'+project_name+'_Z.p','r') as f:\n",
    "    dataNew = cPickle.load(f)\n",
    "    \n",
    "allFrames = []\n",
    "for i in range((len(dataNew)/vae.batch_size)):\n",
    "    newX = vae.generate(dataNew[i*vae.batch_size:(i+1)*vae.batch_size])\n",
    "    allFrames.append(newX)\n",
    "allFrames = np.vstack(np.array(allFrames))\n",
    "allFrames = np.reshape(allFrames,[len(allFrames),128,128,3])\n",
    "\n",
    "for i in range(len(allFrames)):\n",
    "    im = Image.fromarray((allFrames[i] * 256).astype('uint8'))\n",
    "    im.save('./outputs/'+project_name+'/frame'+str(i)+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we combine all the newly generated frames into a video again!\n",
    "\n",
    "`ffmpeg -framerate 10 -i frame%01d.png -c:v libx264 -r 30 -pix_fmt yuv420p out.mp4`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
